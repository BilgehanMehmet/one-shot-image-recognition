{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "from loader import Loader\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from utils import plot,get_val_acc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/Users/mehmetbezcioglu/Documents/SEMESTER1/DeepLearning/one_shot_learning/image_similarity')\n",
    "# from networks.siamese import MNIST\n",
    "# from datasets import Loader\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# from bin.utils import plot,get_val_acc\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "nn = MNIST(shape=(35,35))\n",
    "nn.forward_prop()\n",
    "nn.compute_cost(mode=\"l2_on\")\n",
    "nn.optimize(\"Momentum\")\n",
    "nn.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MNIST:\n",
      " CONV_1: Tensor(\"Relu_4:0\", shape=(?, 33, 33, 96), dtype=float32)\n",
      " POOL_1: Tensor(\"MaxPool_2:0\", shape=(?, 16, 16, 96), dtype=float32)\n",
      " CONV_2: Tensor(\"Relu_5:0\", shape=(?, 12, 12, 256), dtype=float32)\n",
      " POOL_2: Tensor(\"MaxPool_3:0\", shape=(?, 6, 6, 256), dtype=float32)\n",
      " CONV_3: Tensor(\"Relu_6:0\", shape=(?, 4, 4, 256), dtype=float32)\n",
      " FLATTENED:Tensor(\"transpose_1:0\", shape=(4096, ?), dtype=float32)\n",
      ">\n",
      "Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "[<tf.Operation 'Momentum' type=NoOp>, <tf.Operation 'Momentum_1' type=NoOp>, <tf.Operation 'Momentum_2' type=NoOp>, <tf.Operation 'Momentum_3' type=NoOp>, <tf.Operation 'Momentum_4' type=NoOp>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Greater:0' shape=(1, ?) dtype=bool>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nn)\n",
    "print(nn.cost)\n",
    "print(nn.accuracy)\n",
    "print(nn.optimizer)\n",
    "nn.predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Loader(n_examples=100,mode=\"train_split\",path = \"/floyd/input/omniglot_dataset\",normalise=True)\n",
    "test_data  = Loader(n_examples=20,mode=None,path=\"/floyd/input/val_set\",normalise=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(964, 98, 105, 105, 1)\n",
      "(964, 2, 105, 105, 1)\n",
      "(352, 20, 105, 105, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data.X.shape)\n",
    "print(data.X_validation.shape)\n",
    "print(test_data.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the dataset except the training data. (scale the validation set, and the one-shot set only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.X.reshape().shape\n",
    "def rescale(X):\n",
    "    X = X.reshape(-1,105,105)\n",
    "    X.shape\n",
    "    X_rescaled = []\n",
    "    for idx in range(X.shape[0]):\n",
    "        X_rescaled.append(cv2.resize(X[idx],(35,35),interpolation=cv2.INTER_AREA).reshape(35,35,1))\n",
    "    return X_rescaled\n",
    "        \n",
    "# data.X_rescaled = np.asarray(rescale(data.X),dtype=np.float32).reshape(964,-1,35,35,1)\n",
    "data.X_validation_rescaled = np.asarray(rescale(data.X_validation),dtype=np.float32).reshape(964,-1,35,35,1)\n",
    "test_data.X_rescaled = np.asarray(rescale(test_data.X),dtype=np.float32).reshape(352,-1,35,35,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(964, 2, 35, 35, 1)\n",
      "(352, 20, 35, 35, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data.X_validation_rescaled.shape)\n",
    "print(test_data.X_rescaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_one_shot_acc(nn,sess,test_data,val_data,n_trials_per_iter,n_trials):\n",
    "    val_acc = 0\n",
    "    one_shot_acc = 0\n",
    "    for _ in range(n_trials // n_trials_per_iter): \n",
    "        test_imgs, train_imgs, labels = test_data.generateOneShotTrials(test_data.X_validation_rescaled,size=n_trials_per_iter)          \n",
    "        val_acc += get_val_acc(test_imgs,train_imgs,labels,sess,nn,shape=(35,35))\n",
    "        one_shot_test_imgs, one_shot_train_imgs, one_shot_labels = val_data.generateOneShotTrials(val_data.X_rescaled,size=n_trials_per_iter)          \n",
    "        one_shot_acc += get_val_acc(one_shot_test_imgs,one_shot_train_imgs,one_shot_labels,sess,nn,shape=(35,35))\n",
    "    return val_acc/ n_trials, one_shot_acc/ n_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data(epoch,epoch_loss,epoch_acc,val_acc,one_shot_acc):\n",
    "    print(\"Epoch:{}\\t Epoch Loss:{}\\t Train acc:{} \\t Val acc:{}\\t One Shot acc:{}\".format(epoch,epoch_loss,epoch_acc,val_acc,one_shot_acc))\n",
    "    print('{{\"metric\" : \"Loss\", \"value\":{}}}'.format(epoch_loss))\n",
    "    print('{{\"metric\" : \"Training Accuracy\", \"value\":{}}}'.format(epoch_acc))\n",
    "    print('{{\"metric\" : \"Validation Accuracy\", \"value\":{}}}'.format(val_acc))\n",
    "    print('{{\"metric\" : \"One-shot Accuracy\", \"value\":{}}}'.format(one_shot_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nn,hyperparameters,train_data,test_data,val_data,epochs,batch_size,drawer_size):\n",
    "    saver = tf.train.Saver()\n",
    "    filepath = \"/floyd/home/conv_siamese_model\"\n",
    "    seed = 1 \n",
    "    n_trials = 400\n",
    "    n_trials_per_iter = 10\n",
    "    \n",
    "    file = open('/floyd/home/losses_accuracies.txt', 'w')\n",
    "#     test_data.X_validation_rescaled = np.asarray(rescale(test_data.X_validation),dtype=np.float32).reshape(batch_size,35,35,1)\n",
    "#     val_data.X_rescaled = np.asarray(rescale(val_data.X),dtype=np.float32).reshape(batch_size,35,35,1)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # saver.restore(sess,\"/floyd/input/model/conv_siamese_model_40.ckpt\")\n",
    "        # print(\"conv_siamese_model_40.ckpt is restored.\")\n",
    "        for epoch in range(epochs+1):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            training_batch = train_data.get_training_pairs(batch_size= batch_size, drawer_size= drawer_size, seed=seed)\n",
    "            seed += ((train_data.X.shape[0] * train_data.X.shape[1]) // training_batch[0][0].shape[0])       \n",
    "            for counter, batch in enumerate(training_batch):\n",
    "                X1 = np.asarray(rescale(batch[0]),dtype=np.float32).reshape(batch_size,35,35,1)\n",
    "                X2 = np.asarray(rescale(batch[1]),dtype=np.float32).reshape(batch_size,35,35,1)\n",
    "                Y  = batch[2]\n",
    "                _, c, a= sess.run([nn.optimizer,nn.cost,nn.accuracy], feed_dict={\n",
    "                    nn.X:  X1,\n",
    "                    nn.X2: X2,\n",
    "                    nn.Y:  Y,\n",
    "                    nn.learning_rates[0]:hyperparameters[0]   * (0.99** epoch),#0.5\n",
    "                    nn.learning_rates[1]:hyperparameters[1] * (0.99** epoch),\n",
    "                    nn.learning_rates[2]:hyperparameters[2]  * (0.99** epoch),\n",
    "                    nn.learning_rates[3]:hyperparameters[3]  * (0.99** epoch),\n",
    "                    nn.learning_rates[4]:hyperparameters[4]  * (0.99** epoch),\n",
    "                    nn.momentums[0]:hyperparameters[5]  * (1.01** epoch),\n",
    "                    nn.momentums[1]:hyperparameters[6]  * (1.01** epoch),\n",
    "                    nn.momentums[2]:hyperparameters[7]  * (1.01** epoch),\n",
    "                    nn.momentums[3]:hyperparameters[8]  * (1.01** epoch),\n",
    "                    nn.momentums[4]:hyperparameters[9]  * (1.01** epoch)\n",
    "                })\n",
    "                epoch_acc += (a/len(training_batch))\n",
    "                epoch_loss += (c/ len(training_batch))\n",
    "            file.write(str(epoch_loss) + \",\" + str(epoch_acc) +   \"\\n\")\n",
    "            val_acc, one_shot_acc = calc_one_shot_acc(nn,sess,test_data,val_data,n_trials_per_iter,n_trials)\n",
    "            display_data(epoch,epoch_loss,epoch_acc,val_acc,one_shot_acc)\n",
    "            if ((epoch % 10 == 0) and epoch != 0):\n",
    "                print(\"Saving the model...\")\n",
    "                saver.save(sess,filepath + \"_\" + str(epoch) + \".ckpt\")\n",
    "        file.close() \n",
    "    return one_shot_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\t Epoch Loss:0.43554883004042605\t Train acc:0.7988863482384827 \t Val acc:0.72\t One Shot acc:0.6275\n",
      "{\"metric\" : \"Loss\", \"value\":0.43554883004042605}\n",
      "{\"metric\" : \"Training Accuracy\", \"value\":0.7988863482384827}\n",
      "{\"metric\" : \"Validation Accuracy\", \"value\":0.72}\n",
      "{\"metric\" : \"One-shot Accuracy\", \"value\":0.6275}\n",
      "Epoch:1\t Epoch Loss:0.2510175024005297\t Train acc:0.9004065040650404 \t Val acc:0.825\t One Shot acc:0.7175\n",
      "{\"metric\" : \"Loss\", \"value\":0.2510175024005297}\n",
      "{\"metric\" : \"Training Accuracy\", \"value\":0.9004065040650404}\n",
      "{\"metric\" : \"Validation Accuracy\", \"value\":0.825}\n",
      "{\"metric\" : \"One-shot Accuracy\", \"value\":0.7175}\n",
      "Epoch:2\t Epoch Loss:0.19013811989124554\t Train acc:0.927411500677506 \t Val acc:0.85\t One Shot acc:0.7625\n",
      "{\"metric\" : \"Loss\", \"value\":0.19013811989124554}\n",
      "{\"metric\" : \"Training Accuracy\", \"value\":0.927411500677506}\n",
      "{\"metric\" : \"Validation Accuracy\", \"value\":0.85}\n",
      "{\"metric\" : \"One-shot Accuracy\", \"value\":0.7625}\n",
      "Epoch:3\t Epoch Loss:0.15758605379116558\t Train acc:0.941501524390242 \t Val acc:0.8825\t One Shot acc:0.8325\n",
      "{\"metric\" : \"Loss\", \"value\":0.15758605379116558}\n",
      "{\"metric\" : \"Training Accuracy\", \"value\":0.941501524390242}\n",
      "{\"metric\" : \"Validation Accuracy\", \"value\":0.8825}\n",
      "{\"metric\" : \"One-shot Accuracy\", \"value\":0.8325}\n",
      "Epoch:4\t Epoch Loss:0.13890065794411877\t Train acc:0.9496739498644972 \t Val acc:0.8825\t One Shot acc:0.8175\n",
      "{\"metric\" : \"Loss\", \"value\":0.13890065794411877}\n",
      "{\"metric\" : \"Training Accuracy\", \"value\":0.9496739498644972}\n",
      "{\"metric\" : \"Validation Accuracy\", \"value\":0.8825}\n",
      "{\"metric\" : \"One-shot Accuracy\", \"value\":0.8175}\n"
     ]
    }
   ],
   "source": [
    "# train_data = Loader(n_examples=100,mode=\"train_split\",path=\"/floyd/input/omniglot_dataset\",normalise= True)\n",
    "# test_data  = Loader(n_examples=20,mode=None,path=\"/floyd/input/val_set\",normalise=True) \n",
    "hyperparameters = [\n",
    "                    0.1,\n",
    "                    0.1,\n",
    "                    0.1,\n",
    "                    0.05,\n",
    "                    0.01,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    0.5\n",
    "          ]\n",
    "acc = train(nn,hyperparameters,data,data,test_data,epochs=50,batch_size=128,drawer_size=2)\n",
    "print(\"Final one shot accuracy:{}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
