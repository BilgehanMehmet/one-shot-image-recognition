{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Neural Network Tutorial on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "np.random.randint(0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'm'), (2, 'b'), (3, 'c')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'm'), (2, 'b'), (3, 'c')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = ['m','b','c']\n",
    "print(list(zip(a,b)))\n",
    "[_ for _ in zip(a,b)]\n",
    "# [i for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[0 1]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# for _ in range(10):\n",
    "#     print(np.random.randint(0,2,2))\n",
    "for _ in range(10):\n",
    "    print(\"{}\".format(np.random.permutation(2)))\n",
    "a = np.random.permutation(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.970299, 1.940598, 2.910897]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2,3]])\n",
    "a * (0.99 ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import mnist\n",
    "# from utilities import *\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "cost = tf.add(tf.add(w ** 2,tf.multiply(-10.,w)),25)\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "print(sess.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"GradientDescent_5\"\n",
      "op: \"NoOp\"\n",
      "input: \"^GradientDescent_5/update_Variable_5/ApplyGradientDescent\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess.run(train)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.479656"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9999886\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    sess.run(train)\n",
    "print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Tensorflow\n",
    "\n",
    "-  initialize variables\n",
    "-  create a session\n",
    "-  run the operations inside the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int64)\n",
    "print(sess.run(x,feed_dict = {x:3}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(3)\n",
    "b = tf.constant(4)\n",
    "c = tf.multiply(a, b)\n",
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Function\n",
    "\n",
    "Compute Y= WX + b where W and X are random matrices and b is a random vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:\n",
      "[[-2.15657382]\n",
      " [ 2.95891446]\n",
      " [-1.08926781]\n",
      " [-0.84538042]]\n",
      "[[-2.15657382]\n",
      " [ 2.95891446]\n",
      " [-1.08926781]\n",
      " [-0.84538042]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.25949646, 0.03511903, 0.70538451]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = tf.constant(np.random.randn(3,1))\n",
    "W = tf.constant(np.random.randn(4,3))\n",
    "b = tf.constant(np.random.randn(4,1))\n",
    "Y = tf.add(tf.matmul(W,X),b)\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run(Y)\n",
    "# sess.close()\n",
    "\n",
    "print(\"Y:\\n{}\".format(result))\n",
    "print(sess.run(Y))\n",
    "np.exp(np.array([[5,3,6]]))/ np.sum(np.exp(np.array([[5,3,6]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing sigmoid\n",
    "\n",
    "-  tf.sigmoid\n",
    "-  tf.softmax\n",
    "\n",
    "this exercise will be completed using a placeholder variable x. note:\n",
    "-  when using placeholders we feed data into the placeholder using feed dictionary.\n",
    "\n",
    "-  create a placeholder x\n",
    "-  define the operations needed to compute the sigmoid using tf.sigmoid()\n",
    "-  then run the session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "sigmoid = tf.sigmoid(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(sigmoid,feed_dict ={x:0})\n",
    "    \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59813887 0.91301525 0.974077  ]]\n"
     ]
    }
   ],
   "source": [
    "z = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=y)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    cost = session.run(cost,feed_dict={\n",
    "        z:np.array([[0.2,0.4,0.5]]),\n",
    "        y:np.array([[1,0,0]])\n",
    "    })\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "-  tf.one_hot(labels,depth,axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETED THE TF.ONE_HOT FUNCTION\n",
    "\n",
    "def one_hot_encode_labels(Y,depth):\n",
    "    assert (Y.shape == (1,10000)) or (Y.shape == (1,60000))\n",
    "    temp = np.zeros([depth,Y.shape[1]])\n",
    "    for j in range(Y.shape[1]):\n",
    "        temp[Y[0,j],j] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize with zeros and ones\n",
    "\n",
    "-  use tf.ones(shape)\n",
    "-  tf.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "ones = tf.zeros([3,1])\n",
    "sess = tf.Session()\n",
    "ones = sess.run(ones)\n",
    "sess.close()\n",
    "print(ones.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building neural network for MNIST\n",
    "\n",
    "Functions:\n",
    "-  `one_hot_encode()`\n",
    "-  `create_placeholders()`\n",
    "-  `initialize_parameters()`\n",
    "-  `forward_prop()`\n",
    "-  `compute_cost()`\n",
    "-  `back_prop()`\n",
    "-  `model()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import MNIST + one-hot encode labels\n",
    "\n",
    "-  import the training data\n",
    "-  one-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "(784, 60000)\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train = mnist.train_images().reshape(60000,-1).T,mnist.train_labels().reshape(1,-1)\n",
    "plt.imshow(x_train[:,0].reshape(28,28),cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "y_train = one_hot_encode_labels(Y = y_train,depth=10)\n",
    "print(y_train.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = mnist.train_images()\n",
    "a.shape\n",
    "# a.reshape(a.shape[0],-1).shape\n",
    "plt.imshow(a[0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x,n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the session.\n",
    "    \n",
    "    Arguments:\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "    Tips:\n",
    "    -  None means the number of examples you will pass is flexible.\n",
    "    \n",
    "    \"\"\"\n",
    "    return (\n",
    "        tf.placeholder(dtype=\"float\",shape=(n_x,None)),\n",
    "        tf.placeholder(dtype=\"float\",shape=(n_y,None)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_placeholders(n_x=X.shape[0],n_y=Y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise network parameters\n",
    "\n",
    "-  `tf.get_variable(\"W1\",[#rows,#columns],initializer=tf.contrib.layers.xavier_initializer(seed=1))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_parameters(layers):\n",
    "    \"\"\"\n",
    "    Initialises the parameters to build a network with depth of layers.\n",
    "    \n",
    "    Arguments:\n",
    "        layers - list of number of activation units in each layer including the input layer\n",
    "               - [X,#layer1,#layer2,#layer3]\n",
    "        \n",
    "    Returns:\n",
    "        a dictionary of tensors containing the network parameters\n",
    "    \"\"\"\n",
    "    \n",
    "#     tf.set_random_seed(1)\n",
    "    return {\n",
    "        \"W1\":tf.get_variable(\"W1\",[layers[1],layers[0]],initializer = tf.contrib.layers.xavier_initializer(seed=1)),\n",
    "        \"b1\":tf.get_variable(\"b1\",[layers[1],1],initializer = tf.contrib.layers.xavier_initializer(seed=1)),\n",
    "        \"W2\":tf.get_variable(\"W2\",[layers[2],layers[1]],initializer = tf.contrib.layers.xavier_initializer(seed=1)),\n",
    "        \"b2\":tf.get_variable(\"b2\",[layers[2],1],initializer = tf.contrib.layers.xavier_initializer(seed=1)),\n",
    "        \"W3\":tf.get_variable(\"W3\",[layers[3],layers[2]],initializer = tf.contrib.layers.xavier_initializer(seed=1)),\n",
    "        \"b3\":tf.get_variable(\"b3\",[layers[3],1],initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W3:0' shape=(10, 12) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # use this line to reset the graph so the re-creation of tf variables dont throw a ValueError.\n",
    "parameters = initialise_parameters([X.shape[0],25,12,10])\n",
    "print(parameters[\"W3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "-  `tf.matmul`\n",
    "-  `tf.add`\n",
    "-  `tf.nn.relu`\n",
    "\n",
    "\n",
    "-  <font color='red'> **Note: we don't apply activation function on the logits** </font>\n",
    "-  Also, no need to return the layers as we only need the logits for back prop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,parameters):\n",
    "    \"\"\"\n",
    "    Implements forward prop for the model: LINEAR-> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "   \n",
    "   \n",
    "    Arguments:\n",
    "        - X: input data of shape (784,None)\n",
    "        - parameters: dictionary of network parameters\n",
    "        \n",
    "    Returns:\n",
    "        - logits: last layers neurons without activation functions applied.\n",
    "    \"\"\"\n",
    "    A1 = tf.nn.relu(\n",
    "        tf.add(\n",
    "            tf.matmul(\n",
    "                parameters[\"W1\"],X),\n",
    "                parameters[\"b1\"]\n",
    "        )\n",
    "    )\n",
    "    A2 = tf.nn.relu(\n",
    "        tf.add(\n",
    "            tf.matmul(parameters[\"W2\"],A1),\n",
    "            parameters[\"b2\"]\n",
    "        )\n",
    "    )\n",
    "    return tf.add(\n",
    "        tf.matmul(parameters[\"W3\"],A2),parameters[\"b3\"]\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FailedPreconditionError arises because the program is attempting to read a variable (named \"Variable_1\") before it has been initialized. In TensorFlow, all variables must be explicitly initialized, by running their \"initializer\" operations. For convenience, you can run all of the variable initializers in the current session by executing the following statement before your training loop:\n",
    "\n",
    "tf.initialize_all_variables().run()\n",
    "Note that this answer assumes that, as in the question, you are using tf.InteractiveSession, which allows you to run operations without specifying a session. For non-interactive uses, it is more common to use tf.Session, and initialize as follows:\n",
    "\n",
    "`init_op = tf.initialize_all_variables()`\n",
    "\n",
    "`sess = tf.Session()`\n",
    "`sess.run(init_op)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, ?)\n",
      "(10, ?)\n",
      "(10, ?)\n",
      "1\n",
      "[[0.00000000e+00 1.06826025e-17]\n",
      " [8.95857058e-32 1.42374512e-22]\n",
      " [3.32085573e-29 3.36789642e-27]\n",
      " [2.22648851e-24 9.49730559e-28]\n",
      " [3.43781561e-38 0.00000000e+00]\n",
      " [8.93594150e-17 2.41631730e-29]\n",
      " [1.00000000e+00 1.00000000e+00]\n",
      " [0.00000000e+00 4.85515832e-19]\n",
      " [0.00000000e+00 1.26751302e-15]\n",
      " [5.89723870e-27 2.11766612e-19]]\n",
      "[[0.0000000e+00 1.0682602e-17]\n",
      " [8.9585876e-32 1.4237445e-22]\n",
      " [3.3208494e-29 3.3678980e-27]\n",
      " [2.2264919e-24 9.4973229e-28]\n",
      " [3.4378092e-38 0.0000000e+00]\n",
      " [8.9359250e-17 2.4163242e-29]\n",
      " [1.0000000e+00 1.0000000e+00]\n",
      " [0.0000000e+00 4.8551676e-19]\n",
      " [0.0000000e+00 1.2675106e-15]\n",
      " [5.8972333e-27 2.1176701e-19]]\n",
      "(10, ?)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X, Y = create_placeholders(n_x=x_train.shape[0],n_y=y_train.shape[0])\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "parameters = initialise_parameters([X.shape[0],25,12,10])\n",
    "logits = forward_prop(X,parameters)\n",
    "logits2 = forward_prop(X,parameters)\n",
    "print(logits.shape)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sftmax,l_exp = sess.run([\n",
    "                        tf.nn.softmax(logits[:,0:2],axis=0),\n",
    "                        tf.exp(logits[:,0:2]) / tf.reduce_sum(tf.exp(logits[:,0:2]),axis=0)\n",
    "                        ],\n",
    "                        feed_dict=  {\n",
    "                            X:x_train,Y:y_train  \n",
    "                                    })\n",
    "    logits1,logits2 = sess.run ([logits,logits2], feed_dict={X:x_train, Y:y_train})\n",
    "    \n",
    "    w = 1\n",
    "print(w)\n",
    "print(sftmax[:,0:2])\n",
    "print(l_exp[:,0:2])\n",
    "print(logits.shape)\n",
    "\n",
    "# a = tf.get_variable(\"w\",[1])\n",
    "# b = tf.get_variable(\"w\",[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]\n",
      " [ True  True]]\n"
     ]
    }
   ],
   "source": [
    "print(logits1[:,:2] == logits2[:,:2])\n",
    "# print(logits2[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "q = sess.run(tf.nn.softmax(np.array([[0.2,0.2,0.6],[0.3,0.6,0.1]]),axis=0))\n",
    "t = np.array([[0.2,0.2,0.6],[0.3,0.6,0.1]])\n",
    "w = sess.run(tf.exp(t) / tf.reduce_sum(tf.exp(t),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47502081, 0.40131234, 0.62245933],\n",
       "       [0.52497919, 0.59868766, 0.37754067]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Cost\n",
    "\n",
    "- `tf.nn.softmax_cross_entropy_with_logits(logits=...,labels=...)`\n",
    "\n",
    "\n",
    "- **NOTE: inputs of this function are expected to be of shape(#examples,#classes). Thus you need to transpose logits, and Y before computing the cost.** \n",
    "\n",
    "\n",
    "-  `tf.reduce_mean does the summation over the examples.`\n",
    "\n",
    "- note when running sess.run if we dont pass placeholders we dont need to use feed_dict.\n",
    "- else, if we are using placeholders somewhere in the code then it is necessary to pass values to placeholders using feed_dict{}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(logits,Y):\n",
    "    \"\"\"\n",
    "    Computes the cost for the network.\n",
    "    \n",
    "    Arguments:\n",
    "        - logits of the last layer\n",
    "        - labels\n",
    "    \n",
    "    Returns:\n",
    "        - Softmax cross entropy\n",
    "    \"\"\"\n",
    "    \n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(logits),labels=tf.transpose(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l:[[0.2 0.4 0.1]\n",
      " [0.3 0.2 0.1]\n",
      " [0.5 0.4 0.8]]\n",
      "y:[[1 0 1]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112.45628"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = np.array([[0.2,0.4,0.1],[0.3,0.2,0.1],[0.5,0.4,0.8]])\n",
    "y = np.array([[1,0,1],[0,1,0],[0,0,0]])\n",
    "print(\"l:{}\".format(l))\n",
    "print(\"y:{}\".format(y))\n",
    "cost = compute_cost(l,y)\n",
    "cost_with_placeholders = compute_cost(logits,Y)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(cost)\n",
    "sess.run(cost_with_placeholders,feed_dict ={\n",
    "    X:x_train,\n",
    "    Y:y_train\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation + Parameter updates\n",
    "\n",
    "After computing the **cost function** you will need to create an **optimiser** object.\n",
    "\n",
    "-  Create an initialiser object.\n",
    "-  Call it using sess.run()\n",
    "\n",
    "For instance for grad descent optimizer we would do:\n",
    "\n",
    "\n",
    "`optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)`\n",
    "\n",
    "To actually evaluate the optimizer do:\n",
    "\n",
    "`_, c = sess.run(\n",
    "        [optimizer,cost],\n",
    "        feed_dict = {\n",
    "            X:minibatch_X,\n",
    "            Y:minibatch_Y\n",
    "        }\n",
    "    )`\n",
    "    \n",
    "-  Note we use _ to throwaway the evaluated value of optimizer as we don't need to use it.\n",
    "-  c takes the value of the cost returned upon evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.2245077   0.09418039 -0.28316929 -0.39571274 -0.71243197]\n",
      " [-1.35634559 -0.8377157  -0.49391067 -0.14144644 -0.87467077]\n",
      " [ 1.0468758   0.26983708  0.40891533 -0.33733696  0.11074674]\n",
      " [ 0.57440022  0.68663023  1.72377011 -2.42197115 -1.50118296]]\n",
      "[[ 0.57440022  0.68663023  1.72377011 -2.42197115 -1.50118296]\n",
      " [-1.35634559 -0.8377157  -0.49391067 -0.14144644 -0.87467077]\n",
      " [ 1.0468758   0.26983708  0.40891533 -0.33733696  0.11074674]\n",
      " [ 1.2245077   0.09418039 -0.28316929 -0.39571274 -0.71243197]]\n"
     ]
    }
   ],
   "source": [
    "np.random.permutation(11)\n",
    "a = np.random.randn(4,5)\n",
    "print(a)\n",
    "print(a[np.random.permutation(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_batch(X,Y,seed):\n",
    "    np.random.seed(seed)\n",
    "    permutation = np.random.permutation(X.shape[1])\n",
    "    shuffled_x = X[:,permutation]\n",
    "    shuffled_y = Y[:,permutation]\n",
    "    return getBatch(128, shuffled_x, shuffled_y)\n",
    "#     plt.imshow(batch[0][0][:,0].reshape(28,28),cmap='gray')\n",
    "#     plt.show()\n",
    "#     batch[0][1][:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 2\n",
    "batch = get_random_batch(X = x_train, Y = y_train, seed=seed)\n",
    "plt.imshow(batch[0][0][:,0].reshape(28,28),cmap='gray')\n",
    "plt.show()\n",
    "batch[0][1][:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train,Y_train,x_test,y_test,learning_rate=1e-4,epochs=1000,minibatch_size=128):\n",
    "    \"\"\"\n",
    "    Builds the a neural network and trains it.\n",
    "    LINEAR-> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Number of logits in the output layer: 10\n",
    "    \n",
    "    Arguments:\n",
    "        - X_train -> training input of shape (#input features = 784, #training examples = 60000)\n",
    "        - Y_train -> training labels of shape (#classes = 10, # training labels = 60000)\n",
    "        - hyperparameters: learning_rate, epochs, minibatch_size\n",
    "    \n",
    "    Returns:\n",
    "        - trained parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                           #reset the graph to prevent overwriting tf variables\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    __NUMBER_OF_CLASSESS = Y_train.shape[0]\n",
    "    __NUMBER_OF_FEATURES = X_train.shape[0]\n",
    "    __LAYERS = [__NUMBER_OF_FEATURES,25,12,10]\n",
    "    \n",
    "    \n",
    "    \n",
    "# create placeholders\n",
    "# initialise parameters\n",
    "# forward prop\n",
    "# compute cost\n",
    "# optimize\n",
    "\n",
    "# initialize global variables\n",
    "# for each loop do:\n",
    "#     run optimizer\n",
    "    \n",
    "    X, Y = create_placeholders(n_x = __NUMBER_OF_FEATURES,n_y = __NUMBER_OF_CLASSESS)\n",
    "    parameters = initialise_parameters(__LAYERS)\n",
    "    logits = forward_prop(X = X ,parameters = parameters)\n",
    "    cost = compute_cost(logits = logits, Y = Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    softmax = tf.nn.softmax(logits,axis=0)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(softmax,axis=0),tf.argmax(Y)),'float'))\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    seed = 1\n",
    "    costs = []\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_cost = 0\n",
    "            trainingBatch = get_random_batch(X = X_train, Y = Y_train, seed = seed + 1)\n",
    "            __NUMBER_OF_MINIBATCHES = len(trainingBatch)\n",
    "            for batch in trainingBatch:\n",
    "                _ , minibatch_cost = sess.run(\n",
    "                            [optimizer,cost],\n",
    "                            feed_dict ={\n",
    "                                X: batch[0],\n",
    "                                Y: batch[1]\n",
    "                            }\n",
    "                        )\n",
    "                \n",
    "                epoch_cost += minibatch_cost\n",
    "            costs.append(epoch_cost / __NUMBER_OF_MINIBATCHES)\n",
    "            print(\"Epoch:{}\\t Cost:{}\".format(epoch,epoch_cost / __NUMBER_OF_MINIBATCHES))\n",
    "        training_accuracy = sess.run(accuracy, feed_dict ={\n",
    "            X:x_train,\n",
    "            Y:y_train\n",
    "        })\n",
    "        test_accuracy = sess.run(accuracy, feed_dict ={\n",
    "            X:x_test,\n",
    "            Y:y_test\n",
    "        })\n",
    "        print(\"\\n\\nTraining accuracy:{}\".format(training_accuracy))\n",
    "        print(\"Test accuracy:{}\".format(test_accuracy))\n",
    "        plt.plot(np.squeeze(costs)) \n",
    "        return sess.run(parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\t Cost:8.500820648949793\n",
      "Epoch:1\t Cost:2.2645554839929285\n",
      "Epoch:2\t Cost:2.012695349101573\n",
      "Epoch:3\t Cost:1.8437404159798043\n",
      "Epoch:4\t Cost:1.7498045620887772\n",
      "Epoch:5\t Cost:1.6831159614550788\n",
      "Epoch:6\t Cost:1.6233491285015016\n",
      "Epoch:7\t Cost:1.552629985280637\n",
      "Epoch:8\t Cost:1.4985240194589091\n",
      "Epoch:9\t Cost:1.4519597792676262\n",
      "Epoch:10\t Cost:1.4076689278393157\n",
      "Epoch:11\t Cost:1.3127902225136503\n",
      "Epoch:12\t Cost:1.2172946364386503\n",
      "Epoch:13\t Cost:1.1544798756204944\n",
      "Epoch:14\t Cost:1.1084368903753854\n",
      "Epoch:15\t Cost:1.0682361823663529\n",
      "Epoch:16\t Cost:1.032265441885381\n",
      "Epoch:17\t Cost:1.0004065536232645\n",
      "Epoch:18\t Cost:0.9680245119625571\n",
      "Epoch:19\t Cost:0.9415585840943017\n",
      "Epoch:20\t Cost:0.9131652531084984\n",
      "Epoch:21\t Cost:0.8860515206099064\n",
      "Epoch:22\t Cost:0.8619917057978828\n",
      "Epoch:23\t Cost:0.8384449328186669\n",
      "Epoch:24\t Cost:0.8137998672436549\n",
      "Epoch:25\t Cost:0.7905214821606048\n",
      "Epoch:26\t Cost:0.7714424184136299\n",
      "Epoch:27\t Cost:0.7535698509165473\n",
      "Epoch:28\t Cost:0.7375083096754322\n",
      "Epoch:29\t Cost:0.7213174871035984\n",
      "Epoch:30\t Cost:0.706390077371333\n",
      "Epoch:31\t Cost:0.6926222905548397\n",
      "Epoch:32\t Cost:0.6801385426445048\n",
      "Epoch:33\t Cost:0.6684259521300351\n",
      "Epoch:34\t Cost:0.656225641716772\n",
      "Epoch:35\t Cost:0.6446492645277906\n",
      "Epoch:36\t Cost:0.633168675180183\n",
      "Epoch:37\t Cost:0.6225588272121161\n",
      "Epoch:38\t Cost:0.6118341998886198\n",
      "Epoch:39\t Cost:0.5995061684137722\n",
      "Epoch:40\t Cost:0.5877698578560022\n",
      "Epoch:41\t Cost:0.5759754131979017\n",
      "Epoch:42\t Cost:0.5643328706592893\n",
      "Epoch:43\t Cost:0.551978865030732\n",
      "Epoch:44\t Cost:0.5405174964018213\n",
      "Epoch:45\t Cost:0.5283459358250917\n",
      "Epoch:46\t Cost:0.5176260475156658\n",
      "Epoch:47\t Cost:0.5076118715282188\n",
      "Epoch:48\t Cost:0.49840162017706363\n",
      "Epoch:49\t Cost:0.4905268411392342\n",
      "Epoch:50\t Cost:0.48174010588924515\n",
      "Epoch:51\t Cost:0.4731010433071966\n",
      "Epoch:52\t Cost:0.46422957872022697\n",
      "Epoch:53\t Cost:0.4562914195472498\n",
      "Epoch:54\t Cost:0.44890855014451275\n",
      "Epoch:55\t Cost:0.44151293703996297\n",
      "Epoch:56\t Cost:0.43400433780287884\n",
      "Epoch:57\t Cost:0.4284059731945046\n",
      "Epoch:58\t Cost:0.42348543648272435\n",
      "Epoch:59\t Cost:0.4169179091829735\n",
      "Epoch:60\t Cost:0.4122092760384464\n",
      "Epoch:61\t Cost:0.4076182476556632\n",
      "Epoch:62\t Cost:0.4032767552620312\n",
      "Epoch:63\t Cost:0.3993158154904461\n",
      "Epoch:64\t Cost:0.3946409993715632\n",
      "Epoch:65\t Cost:0.38971387024627313\n",
      "Epoch:66\t Cost:0.38656412055497485\n",
      "Epoch:67\t Cost:0.38200039071823233\n",
      "Epoch:68\t Cost:0.37862456442196485\n",
      "Epoch:69\t Cost:0.375378054787101\n",
      "Epoch:70\t Cost:0.3727597595532057\n",
      "Epoch:71\t Cost:0.36892207268712873\n",
      "Epoch:72\t Cost:0.3647618006541531\n",
      "Epoch:73\t Cost:0.3617129456450436\n",
      "Epoch:74\t Cost:0.358136332270179\n",
      "Epoch:75\t Cost:0.35536283181547357\n",
      "Epoch:76\t Cost:0.35185601258837085\n",
      "Epoch:77\t Cost:0.3481251178964623\n",
      "Epoch:78\t Cost:0.34052102131121703\n",
      "Epoch:79\t Cost:0.3313816005169456\n",
      "Epoch:80\t Cost:0.3221511842726644\n",
      "Epoch:81\t Cost:0.3137967759993539\n",
      "Epoch:82\t Cost:0.3054326685951717\n",
      "Epoch:83\t Cost:0.2981824237527624\n",
      "Epoch:84\t Cost:0.2916745918391864\n",
      "Epoch:85\t Cost:0.2858917716342503\n",
      "Epoch:86\t Cost:0.2801227457741939\n",
      "Epoch:87\t Cost:0.27405717813256963\n",
      "Epoch:88\t Cost:0.26909971523132403\n",
      "Epoch:89\t Cost:0.26304199301929615\n",
      "Epoch:90\t Cost:0.25855753284845273\n",
      "Epoch:91\t Cost:0.2546620609314203\n",
      "Epoch:92\t Cost:0.25116455864741094\n",
      "Epoch:93\t Cost:0.24682961661678388\n",
      "Epoch:94\t Cost:0.24285971808598747\n",
      "Epoch:95\t Cost:0.23815940716055664\n",
      "Epoch:96\t Cost:0.23474861511480072\n",
      "Epoch:97\t Cost:0.23104491408890498\n",
      "Epoch:98\t Cost:0.22893839505816826\n",
      "Epoch:99\t Cost:0.22620749249577776\n",
      "Epoch:100\t Cost:0.2229580878417121\n",
      "Epoch:101\t Cost:0.21990943858936143\n",
      "Epoch:102\t Cost:0.2166805076859653\n",
      "Epoch:103\t Cost:0.214101858707125\n",
      "Epoch:104\t Cost:0.21220631416100683\n",
      "Epoch:105\t Cost:0.20920083670219633\n",
      "Epoch:106\t Cost:0.2068116300538786\n",
      "Epoch:107\t Cost:0.20405980132853807\n",
      "Epoch:108\t Cost:0.20272299164393817\n",
      "Epoch:109\t Cost:0.19945537005024933\n",
      "Epoch:110\t Cost:0.197042130402474\n",
      "Epoch:111\t Cost:0.19583241268198118\n",
      "Epoch:112\t Cost:0.193541037264282\n",
      "Epoch:113\t Cost:0.19246818933056106\n",
      "Epoch:114\t Cost:0.19014959742646736\n",
      "Epoch:115\t Cost:0.18848329977884984\n",
      "Epoch:116\t Cost:0.1862174376908904\n",
      "Epoch:117\t Cost:0.1857459031855628\n",
      "Epoch:118\t Cost:0.18452826819853232\n",
      "Epoch:119\t Cost:0.1828308523432024\n",
      "Epoch:120\t Cost:0.18146007204614978\n",
      "Epoch:121\t Cost:0.18052020868354007\n",
      "Epoch:122\t Cost:0.17950961283688097\n",
      "Epoch:123\t Cost:0.17790555554444093\n",
      "Epoch:124\t Cost:0.1772201155850501\n",
      "Epoch:125\t Cost:0.17535830721227344\n",
      "Epoch:126\t Cost:0.17454666056549117\n",
      "Epoch:127\t Cost:0.1728935420179545\n",
      "Epoch:128\t Cost:0.17225443491581152\n",
      "Epoch:129\t Cost:0.17141284533877615\n",
      "Epoch:130\t Cost:0.17076653945071102\n",
      "Epoch:131\t Cost:0.16945550883057783\n",
      "Epoch:132\t Cost:0.1678173051142235\n",
      "Epoch:133\t Cost:0.1678605221990329\n",
      "Epoch:134\t Cost:0.16681772750863896\n",
      "Epoch:135\t Cost:0.16525755060125769\n",
      "Epoch:136\t Cost:0.16399973849339017\n",
      "Epoch:137\t Cost:0.16338166647723742\n",
      "Epoch:138\t Cost:0.16047384735268316\n",
      "Epoch:139\t Cost:0.16036471763431137\n",
      "Epoch:140\t Cost:0.16004879886248727\n",
      "Epoch:141\t Cost:0.15910170092257356\n",
      "Epoch:142\t Cost:0.1602542811571789\n",
      "Epoch:143\t Cost:0.1582999938348336\n",
      "Epoch:144\t Cost:0.1559903027056885\n",
      "Epoch:145\t Cost:0.15535296478282923\n",
      "Epoch:146\t Cost:0.1554174512974235\n",
      "Epoch:147\t Cost:0.15443398111632892\n",
      "Epoch:148\t Cost:0.15306877197106\n",
      "Epoch:149\t Cost:0.1528505533218765\n",
      "Epoch:150\t Cost:0.1517607048392169\n",
      "Epoch:151\t Cost:0.15068847450938053\n",
      "Epoch:152\t Cost:0.1499791837482056\n",
      "Epoch:153\t Cost:0.1487190572739537\n",
      "Epoch:154\t Cost:0.1479024541403439\n",
      "Epoch:155\t Cost:0.1480178796310923\n",
      "Epoch:156\t Cost:0.14879990852018918\n",
      "Epoch:157\t Cost:0.14808441508712292\n",
      "Epoch:158\t Cost:0.1453617913509483\n",
      "Epoch:159\t Cost:0.14519012294439618\n",
      "Epoch:160\t Cost:0.14446312081076698\n",
      "Epoch:161\t Cost:0.1451259553353034\n",
      "Epoch:162\t Cost:0.1433434992758577\n",
      "Epoch:163\t Cost:0.14350390651110392\n",
      "Epoch:164\t Cost:0.14268900442129767\n",
      "Epoch:165\t Cost:0.1419942658831443\n",
      "Epoch:166\t Cost:0.14038027778490267\n",
      "Epoch:167\t Cost:0.1405969835055281\n",
      "Epoch:168\t Cost:0.14105041767917334\n",
      "Epoch:169\t Cost:0.13937828913410463\n",
      "Epoch:170\t Cost:0.1400316326475855\n",
      "Epoch:171\t Cost:0.14010905306826013\n",
      "Epoch:172\t Cost:0.13854067553795857\n",
      "Epoch:173\t Cost:0.137470920175028\n",
      "Epoch:174\t Cost:0.13837311990352583\n",
      "Epoch:175\t Cost:0.13695974724252086\n",
      "Epoch:176\t Cost:0.1377211408908052\n",
      "Epoch:177\t Cost:0.1359122023780717\n",
      "Epoch:178\t Cost:0.13573971069824975\n",
      "Epoch:179\t Cost:0.13596038144629902\n",
      "Epoch:180\t Cost:0.13549347949831853\n",
      "Epoch:181\t Cost:0.13423171064365647\n",
      "Epoch:182\t Cost:0.13302409627051878\n",
      "Epoch:183\t Cost:0.13355596493413327\n",
      "Epoch:184\t Cost:0.134996790843986\n",
      "Epoch:185\t Cost:0.13472053146899254\n",
      "Epoch:186\t Cost:0.13282070253719527\n",
      "Epoch:187\t Cost:0.13228516303288784\n",
      "Epoch:188\t Cost:0.130195531370575\n",
      "Epoch:189\t Cost:0.13303824746881976\n",
      "Epoch:190\t Cost:0.13099776465755536\n",
      "Epoch:191\t Cost:0.13162895616120113\n",
      "Epoch:192\t Cost:0.13140292217466495\n",
      "Epoch:193\t Cost:0.13081194699477794\n",
      "Epoch:194\t Cost:0.13032102947439084\n",
      "Epoch:195\t Cost:0.12980198643323201\n",
      "Epoch:196\t Cost:0.1297748145430899\n",
      "Epoch:197\t Cost:0.12935893307489627\n",
      "Epoch:198\t Cost:0.12834184101697352\n",
      "Epoch:199\t Cost:0.127965909784346\n",
      "Epoch:200\t Cost:0.1275529076796033\n",
      "Epoch:201\t Cost:0.12976417618233765\n",
      "Epoch:202\t Cost:0.12723239937793218\n",
      "Epoch:203\t Cost:0.12693043393509856\n",
      "Epoch:204\t Cost:0.129631964311099\n",
      "Epoch:205\t Cost:0.12839676754704035\n",
      "Epoch:206\t Cost:0.1267981185182643\n",
      "Epoch:207\t Cost:0.12634509946825281\n",
      "Epoch:208\t Cost:0.12621975128552806\n",
      "Epoch:209\t Cost:0.1263466118725696\n",
      "Epoch:210\t Cost:0.1265277781768013\n",
      "Epoch:211\t Cost:0.1248303125066353\n",
      "Epoch:212\t Cost:0.12464379066470335\n",
      "Epoch:213\t Cost:0.12484077036158363\n",
      "Epoch:214\t Cost:0.1250275553448368\n",
      "Epoch:215\t Cost:0.126821987402401\n",
      "Epoch:216\t Cost:0.12412981854986026\n",
      "Epoch:217\t Cost:0.1232841468727919\n",
      "Epoch:218\t Cost:0.1229891635970012\n",
      "Epoch:219\t Cost:0.12578136941342594\n",
      "Epoch:220\t Cost:0.12295776617917806\n",
      "Epoch:221\t Cost:0.12413949982872777\n",
      "Epoch:222\t Cost:0.12201872055750412\n",
      "Epoch:223\t Cost:0.12234243926510754\n",
      "Epoch:224\t Cost:0.12111530188065983\n",
      "Epoch:225\t Cost:0.1215509586631934\n",
      "Epoch:226\t Cost:0.1229944298270224\n",
      "Epoch:227\t Cost:0.11976626203226637\n",
      "Epoch:228\t Cost:0.12207844029706932\n",
      "Epoch:229\t Cost:0.12301278084532412\n",
      "Epoch:230\t Cost:0.12302610258271954\n",
      "Epoch:231\t Cost:0.12119780998748503\n",
      "Epoch:232\t Cost:0.11929898882217245\n",
      "Epoch:233\t Cost:0.11856037619382714\n",
      "Epoch:234\t Cost:0.12027298521274316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:235\t Cost:0.1185952531519348\n",
      "Epoch:236\t Cost:0.11790945720491505\n",
      "Epoch:237\t Cost:0.12243159672519419\n",
      "Epoch:238\t Cost:0.1201634887359671\n",
      "Epoch:239\t Cost:0.11902509584053875\n",
      "Epoch:240\t Cost:0.11794952960061367\n",
      "Epoch:241\t Cost:0.11842657055364235\n",
      "Epoch:242\t Cost:0.11780879696978054\n",
      "Epoch:243\t Cost:0.11644140588465149\n",
      "Epoch:244\t Cost:0.11896641501612755\n",
      "Epoch:245\t Cost:0.12053100427966128\n",
      "Epoch:246\t Cost:0.11758702421493368\n",
      "Epoch:247\t Cost:0.11685720352587034\n",
      "Epoch:248\t Cost:0.11968668929732112\n",
      "Epoch:249\t Cost:0.11643580101859341\n",
      "Epoch:250\t Cost:0.11631132466102968\n",
      "Epoch:251\t Cost:0.1169066162966589\n",
      "Epoch:252\t Cost:0.11841982694021039\n",
      "Epoch:253\t Cost:0.11786483450612026\n",
      "Epoch:254\t Cost:0.11603284695112248\n",
      "Epoch:255\t Cost:0.11682026263382007\n",
      "Epoch:256\t Cost:0.1156073475935693\n",
      "Epoch:257\t Cost:0.11530980693776088\n",
      "Epoch:258\t Cost:0.11542449714834375\n",
      "Epoch:259\t Cost:0.11883658437983695\n",
      "Epoch:260\t Cost:0.11556363724537495\n",
      "Epoch:261\t Cost:0.11485793337877244\n",
      "Epoch:262\t Cost:0.11625291172788342\n",
      "Epoch:263\t Cost:0.115387624213055\n",
      "Epoch:264\t Cost:0.11306203405724279\n",
      "Epoch:265\t Cost:0.11344437588697304\n",
      "Epoch:266\t Cost:0.11513544020574612\n",
      "Epoch:267\t Cost:0.1173222937714507\n",
      "Epoch:268\t Cost:0.1153642399026069\n",
      "Epoch:269\t Cost:0.11228291954058828\n",
      "Epoch:270\t Cost:0.11355689552022831\n",
      "Epoch:271\t Cost:0.11424552308899888\n",
      "Epoch:272\t Cost:0.1153970606434447\n",
      "Epoch:273\t Cost:0.11209654920041434\n",
      "Epoch:274\t Cost:0.1131435962024528\n",
      "Epoch:275\t Cost:0.11371442038955083\n",
      "Epoch:276\t Cost:0.11237306998117265\n",
      "Epoch:277\t Cost:0.1113438562138566\n",
      "Epoch:278\t Cost:0.11398319631163627\n",
      "Epoch:279\t Cost:0.11208174648934971\n",
      "Epoch:280\t Cost:0.11241668190306692\n",
      "Epoch:281\t Cost:0.11063496891194696\n",
      "Epoch:282\t Cost:0.11250211031182107\n",
      "Epoch:283\t Cost:0.11457304965093065\n",
      "Epoch:284\t Cost:0.11007466366184927\n",
      "Epoch:285\t Cost:0.10923093835364527\n",
      "Epoch:286\t Cost:0.1107393958345691\n",
      "Epoch:287\t Cost:0.11249020785442802\n",
      "Epoch:288\t Cost:0.11390406118908417\n",
      "Epoch:289\t Cost:0.11209758103433956\n",
      "Epoch:290\t Cost:0.11089511917852389\n",
      "Epoch:291\t Cost:0.11114732484652926\n",
      "Epoch:292\t Cost:0.11135069288408705\n",
      "Epoch:293\t Cost:0.10993966292629619\n",
      "Epoch:294\t Cost:0.10940336219982298\n",
      "Epoch:295\t Cost:0.1099014295253164\n",
      "Epoch:296\t Cost:0.11127854009141037\n",
      "Epoch:297\t Cost:0.11013418473382748\n",
      "Epoch:298\t Cost:0.10811184058740322\n",
      "Epoch:299\t Cost:0.1104085480214468\n",
      "Epoch:300\t Cost:0.10861698504306995\n",
      "Epoch:301\t Cost:0.1105075175407281\n",
      "Epoch:302\t Cost:0.11339788586457274\n",
      "Epoch:303\t Cost:0.10754661017786592\n",
      "Epoch:304\t Cost:0.10663216416118369\n",
      "Epoch:305\t Cost:0.11115302168118801\n",
      "Epoch:306\t Cost:0.10831069962969467\n",
      "Epoch:307\t Cost:0.1063135820649453\n",
      "Epoch:308\t Cost:0.1081017769757968\n",
      "Epoch:309\t Cost:0.10864477883825806\n",
      "Epoch:310\t Cost:0.10855915187311961\n",
      "Epoch:311\t Cost:0.10716213917395453\n",
      "Epoch:312\t Cost:0.1080258202328007\n",
      "Epoch:313\t Cost:0.10756425092072248\n",
      "Epoch:314\t Cost:0.10762763181840306\n",
      "Epoch:315\t Cost:0.10831246503404399\n",
      "Epoch:316\t Cost:0.10684408071730882\n",
      "Epoch:317\t Cost:0.10948591358832586\n",
      "Epoch:318\t Cost:0.10660217502621064\n",
      "Epoch:319\t Cost:0.10730837242229026\n",
      "Epoch:320\t Cost:0.10734615416557931\n",
      "Epoch:321\t Cost:0.10803023565298459\n",
      "Epoch:322\t Cost:0.10746584834256914\n",
      "Epoch:323\t Cost:0.10653372344844886\n",
      "Epoch:324\t Cost:0.10931939874360683\n",
      "Epoch:325\t Cost:0.10689665916473118\n",
      "Epoch:326\t Cost:0.10943054251158352\n",
      "Epoch:327\t Cost:0.10779424407866908\n",
      "Epoch:328\t Cost:0.1063691927993825\n",
      "Epoch:329\t Cost:0.1044165933969369\n",
      "Epoch:330\t Cost:0.10493400824595807\n",
      "Epoch:331\t Cost:0.10654600986094871\n",
      "Epoch:332\t Cost:0.1067843058867368\n",
      "Epoch:333\t Cost:0.10633040261246375\n",
      "Epoch:334\t Cost:0.10766453378355262\n",
      "Epoch:335\t Cost:0.10611222705432474\n",
      "Epoch:336\t Cost:0.10546257715823172\n",
      "Epoch:337\t Cost:0.10375971407857912\n",
      "Epoch:338\t Cost:0.10661338631317878\n",
      "Epoch:339\t Cost:0.10422558439518216\n",
      "Epoch:340\t Cost:0.10345337275407716\n",
      "Epoch:341\t Cost:0.10473740313932903\n",
      "Epoch:342\t Cost:0.10866726180518678\n",
      "Epoch:343\t Cost:0.10257906502267636\n",
      "Epoch:344\t Cost:0.10309087726543707\n",
      "Epoch:345\t Cost:0.10579901625876868\n",
      "Epoch:346\t Cost:0.10789278317401722\n",
      "Epoch:347\t Cost:0.10393805348717455\n",
      "Epoch:348\t Cost:0.10422144145933168\n",
      "Epoch:349\t Cost:0.10260661671569607\n",
      "Epoch:350\t Cost:0.10334470089294635\n",
      "Epoch:351\t Cost:0.10364236799416257\n",
      "Epoch:352\t Cost:0.10471676585119544\n",
      "Epoch:353\t Cost:0.10559647158582582\n",
      "Epoch:354\t Cost:0.10363430824519983\n",
      "Epoch:355\t Cost:0.10203709287731759\n",
      "Epoch:356\t Cost:0.10109000205437639\n",
      "Epoch:357\t Cost:0.10302914138128763\n",
      "Epoch:358\t Cost:0.10394038471665337\n",
      "Epoch:359\t Cost:0.10313898494947693\n",
      "Epoch:360\t Cost:0.1025624903185027\n",
      "Epoch:361\t Cost:0.10144247541597276\n",
      "Epoch:362\t Cost:0.10379447877557992\n",
      "Epoch:363\t Cost:0.10412387321713065\n",
      "Epoch:364\t Cost:0.10144910823180477\n",
      "Epoch:365\t Cost:0.10175740061355616\n",
      "Epoch:366\t Cost:0.1016478392500073\n",
      "Epoch:367\t Cost:0.09979563222916078\n",
      "Epoch:368\t Cost:0.10309697357592171\n",
      "Epoch:369\t Cost:0.10143994395412616\n",
      "Epoch:370\t Cost:0.10280522358204637\n",
      "Epoch:371\t Cost:0.10071088560719861\n",
      "Epoch:372\t Cost:0.10250619690476069\n",
      "Epoch:373\t Cost:0.10179252257503109\n",
      "Epoch:374\t Cost:0.10122567791737981\n",
      "Epoch:375\t Cost:0.10061934609800133\n",
      "Epoch:376\t Cost:0.10138176159182591\n",
      "Epoch:377\t Cost:0.10062989027801353\n",
      "Epoch:378\t Cost:0.10188587056770762\n",
      "Epoch:379\t Cost:0.10163033175776635\n",
      "Epoch:380\t Cost:0.100732337802585\n",
      "Epoch:381\t Cost:0.09847324325649469\n",
      "Epoch:382\t Cost:0.09936774128488005\n",
      "Epoch:383\t Cost:0.09846788199781291\n",
      "Epoch:384\t Cost:0.10139024888338057\n",
      "Epoch:385\t Cost:0.10229552268330604\n",
      "Epoch:386\t Cost:0.09921249816777991\n",
      "Epoch:387\t Cost:0.09751803295484293\n",
      "Epoch:388\t Cost:0.09649005617453918\n",
      "Epoch:389\t Cost:0.10196240013167421\n",
      "Epoch:390\t Cost:0.10070274048236641\n",
      "Epoch:391\t Cost:0.09902443089234486\n",
      "Epoch:392\t Cost:0.09790416672500148\n",
      "Epoch:393\t Cost:0.09770536469196332\n",
      "Epoch:394\t Cost:0.09945352404896639\n",
      "Epoch:395\t Cost:0.10044192746162478\n",
      "Epoch:396\t Cost:0.09884141725914947\n",
      "Epoch:397\t Cost:0.09718368443122297\n",
      "Epoch:398\t Cost:0.09882770021007195\n",
      "Epoch:399\t Cost:0.09992267568307772\n",
      "Epoch:400\t Cost:0.09689924081783495\n",
      "Epoch:401\t Cost:0.09823577051986256\n",
      "Epoch:402\t Cost:0.09907053405248216\n",
      "Epoch:403\t Cost:0.09668232917364662\n",
      "Epoch:404\t Cost:0.0997253423000672\n",
      "Epoch:405\t Cost:0.09750349181237569\n",
      "Epoch:406\t Cost:0.09622931115027429\n",
      "Epoch:407\t Cost:0.0943159648993694\n",
      "Epoch:408\t Cost:0.09711184758526176\n",
      "Epoch:409\t Cost:0.09713000858794334\n",
      "Epoch:410\t Cost:0.09457992123706001\n",
      "Epoch:411\t Cost:0.09567010633425037\n",
      "Epoch:412\t Cost:0.09635087767087702\n",
      "Epoch:413\t Cost:0.1009873692323563\n",
      "Epoch:414\t Cost:0.09744509073621682\n",
      "Epoch:415\t Cost:0.0947743213868567\n",
      "Epoch:416\t Cost:0.09583781692566776\n",
      "Epoch:417\t Cost:0.09986755864095015\n",
      "Epoch:418\t Cost:0.09527595171621486\n",
      "Epoch:419\t Cost:0.09745236356129079\n",
      "Epoch:420\t Cost:0.09468678266667863\n",
      "Epoch:421\t Cost:0.0957081958707144\n",
      "Epoch:422\t Cost:0.09479782490460857\n",
      "Epoch:423\t Cost:0.09696493871700662\n",
      "Epoch:424\t Cost:0.09684314492788078\n",
      "Epoch:425\t Cost:0.09269019354110969\n",
      "Epoch:426\t Cost:0.0969606545795478\n",
      "Epoch:427\t Cost:0.09906474576552095\n",
      "Epoch:428\t Cost:0.09704700685036716\n",
      "Epoch:429\t Cost:0.09456716975701579\n",
      "Epoch:430\t Cost:0.09592997515673386\n",
      "Epoch:431\t Cost:0.09713374980802793\n",
      "Epoch:432\t Cost:0.09486528096406825\n",
      "Epoch:433\t Cost:0.09478544222631816\n",
      "Epoch:434\t Cost:0.09430712389388382\n",
      "Epoch:435\t Cost:0.09341489490685559\n",
      "Epoch:436\t Cost:0.09786353389551836\n",
      "Epoch:437\t Cost:0.09394117445349376\n",
      "Epoch:438\t Cost:0.09372649070963676\n",
      "Epoch:439\t Cost:0.09475610841478684\n",
      "Epoch:440\t Cost:0.09313257624988934\n",
      "Epoch:441\t Cost:0.09444132435129586\n",
      "Epoch:442\t Cost:0.0936204714120737\n",
      "Epoch:443\t Cost:0.09453726061053876\n",
      "Epoch:444\t Cost:0.09444785516844122\n",
      "Epoch:445\t Cost:0.09445861688872645\n",
      "Epoch:446\t Cost:0.09482973933156365\n",
      "Epoch:447\t Cost:0.09397398938399071\n",
      "Epoch:448\t Cost:0.0936154571435313\n",
      "Epoch:449\t Cost:0.09278852901836512\n",
      "Epoch:450\t Cost:0.09230240111126066\n",
      "Epoch:451\t Cost:0.09348045657676801\n",
      "Epoch:452\t Cost:0.09493859994338393\n",
      "Epoch:453\t Cost:0.09374759243781379\n",
      "Epoch:454\t Cost:0.09084722446774178\n",
      "Epoch:455\t Cost:0.0923311818204423\n",
      "Epoch:456\t Cost:0.09369227795132887\n",
      "Epoch:457\t Cost:0.09480522660764931\n",
      "Epoch:458\t Cost:0.0928196939014232\n",
      "Epoch:459\t Cost:0.09462450033446937\n",
      "Epoch:460\t Cost:0.0917729879342226\n",
      "Epoch:461\t Cost:0.09253752645430789\n",
      "Epoch:462\t Cost:0.09291953823245219\n",
      "Epoch:463\t Cost:0.09399336643977714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:464\t Cost:0.09097928548259522\n",
      "Epoch:465\t Cost:0.09316645638504897\n",
      "Epoch:466\t Cost:0.09255646379477879\n",
      "Epoch:467\t Cost:0.09803743167385173\n",
      "Epoch:468\t Cost:0.09237713440617265\n",
      "Epoch:469\t Cost:0.09254911466281233\n",
      "Epoch:470\t Cost:0.09057433346409534\n",
      "Epoch:471\t Cost:0.09310974436266019\n",
      "Epoch:472\t Cost:0.09074304535436922\n",
      "Epoch:473\t Cost:0.09082792141337766\n",
      "Epoch:474\t Cost:0.09174409444366437\n",
      "Epoch:475\t Cost:0.09110300891053702\n",
      "Epoch:476\t Cost:0.09152646978765028\n",
      "Epoch:477\t Cost:0.09239690955966584\n",
      "Epoch:478\t Cost:0.09068451367461605\n",
      "Epoch:479\t Cost:0.09276563040951867\n",
      "Epoch:480\t Cost:0.09354968789194439\n",
      "Epoch:481\t Cost:0.09176778658898845\n",
      "Epoch:482\t Cost:0.08866845823522569\n",
      "Epoch:483\t Cost:0.0904108536666009\n",
      "Epoch:484\t Cost:0.09088470217888989\n",
      "Epoch:485\t Cost:0.09339555685144307\n",
      "Epoch:486\t Cost:0.09128286658144837\n",
      "Epoch:487\t Cost:0.09530091424510359\n",
      "Epoch:488\t Cost:0.0920723373038587\n",
      "Epoch:489\t Cost:0.09004120468727943\n",
      "Epoch:490\t Cost:0.08919222743661483\n",
      "Epoch:491\t Cost:0.09042574559002797\n",
      "Epoch:492\t Cost:0.09113169541117797\n",
      "Epoch:493\t Cost:0.09089078882864035\n",
      "Epoch:494\t Cost:0.09012771601171128\n",
      "Epoch:495\t Cost:0.0893505303276532\n",
      "Epoch:496\t Cost:0.09214561309284175\n",
      "Epoch:497\t Cost:0.0912154621875553\n",
      "Epoch:498\t Cost:0.0915945056460496\n",
      "Epoch:499\t Cost:0.08869335000147062\n",
      "Epoch:500\t Cost:0.0889344821964055\n",
      "Epoch:501\t Cost:0.09019562733103471\n",
      "Epoch:502\t Cost:0.08928729321704364\n",
      "Epoch:503\t Cost:0.09177988578777896\n",
      "Epoch:504\t Cost:0.08926460000950454\n",
      "Epoch:505\t Cost:0.08919658751955736\n",
      "Epoch:506\t Cost:0.09426795219434604\n",
      "Epoch:507\t Cost:0.09284607519203031\n",
      "Epoch:508\t Cost:0.09189005007645659\n",
      "Epoch:509\t Cost:0.08925008544646727\n",
      "Epoch:510\t Cost:0.08833458958499467\n",
      "Epoch:511\t Cost:0.09338309688727929\n",
      "Epoch:512\t Cost:0.08930931265539388\n",
      "Epoch:513\t Cost:0.0918305585248225\n",
      "Epoch:514\t Cost:0.08781666362654171\n",
      "Epoch:515\t Cost:0.08822156894388103\n",
      "Epoch:516\t Cost:0.08877501278512005\n",
      "Epoch:517\t Cost:0.08863472435543977\n",
      "Epoch:518\t Cost:0.09067206416549141\n",
      "Epoch:519\t Cost:0.09046667581126253\n",
      "Epoch:520\t Cost:0.08639652970264843\n",
      "Epoch:521\t Cost:0.08788160021617406\n",
      "Epoch:522\t Cost:0.08790957420937288\n",
      "Epoch:523\t Cost:0.09343714080552366\n",
      "Epoch:524\t Cost:0.08841335146959975\n",
      "Epoch:525\t Cost:0.09103689600092008\n",
      "Epoch:526\t Cost:0.0899091139971924\n",
      "Epoch:527\t Cost:0.08786476096078785\n",
      "Epoch:528\t Cost:0.08779147875382066\n",
      "Epoch:529\t Cost:0.0896221947357821\n",
      "Epoch:530\t Cost:0.08697610669957995\n",
      "Epoch:531\t Cost:0.08715481056706674\n",
      "Epoch:532\t Cost:0.0909348112433704\n",
      "Epoch:533\t Cost:0.08877067911281769\n",
      "Epoch:534\t Cost:0.08856304098707019\n",
      "Epoch:535\t Cost:0.08599253992702979\n",
      "Epoch:536\t Cost:0.08750553479366528\n",
      "Epoch:537\t Cost:0.08846148093348183\n",
      "Epoch:538\t Cost:0.0894179274218836\n",
      "Epoch:539\t Cost:0.08807399598504308\n",
      "Epoch:540\t Cost:0.09041338142301482\n",
      "Epoch:541\t Cost:0.09041356908947801\n",
      "Epoch:542\t Cost:0.08628276023052649\n",
      "Epoch:543\t Cost:0.09021169713922719\n",
      "Epoch:544\t Cost:0.087053959263858\n",
      "Epoch:545\t Cost:0.08786607480276304\n",
      "Epoch:546\t Cost:0.08657570246225799\n",
      "Epoch:547\t Cost:0.08868301160006063\n",
      "Epoch:548\t Cost:0.09122990722309297\n",
      "Epoch:549\t Cost:0.08947039699988134\n",
      "Epoch:550\t Cost:0.08675200267951054\n",
      "Epoch:551\t Cost:0.08660926276655086\n",
      "Epoch:552\t Cost:0.08600577911032416\n",
      "Epoch:553\t Cost:0.08996933561835144\n",
      "Epoch:554\t Cost:0.08695830714934544\n",
      "Epoch:555\t Cost:0.08850367582921408\n",
      "Epoch:556\t Cost:0.08581396956830772\n",
      "Epoch:557\t Cost:0.08697679977633678\n",
      "Epoch:558\t Cost:0.08764960933158965\n",
      "Epoch:559\t Cost:0.08679255589779251\n",
      "Epoch:560\t Cost:0.08764297555862015\n",
      "Epoch:561\t Cost:0.08853442451036943\n",
      "Epoch:562\t Cost:0.09047306371507233\n",
      "Epoch:563\t Cost:0.08628734206554414\n",
      "Epoch:564\t Cost:0.08417042873815687\n",
      "Epoch:565\t Cost:0.08557677060874032\n",
      "Epoch:566\t Cost:0.08970607808634226\n",
      "Epoch:567\t Cost:0.08758090108609212\n",
      "Epoch:568\t Cost:0.0905557579672667\n",
      "Epoch:569\t Cost:0.08440252445511091\n",
      "Epoch:570\t Cost:0.08909421895088544\n",
      "Epoch:571\t Cost:0.08652590099237621\n",
      "Epoch:572\t Cost:0.08671658377705226\n",
      "Epoch:573\t Cost:0.0884865549728592\n",
      "Epoch:574\t Cost:0.08626676724552473\n",
      "Epoch:575\t Cost:0.08688385076105976\n",
      "Epoch:576\t Cost:0.08362712501399298\n",
      "Epoch:577\t Cost:0.08344221606191352\n",
      "Epoch:578\t Cost:0.08765340202859342\n",
      "Epoch:579\t Cost:0.08728301486989329\n",
      "Epoch:580\t Cost:0.0852717289419126\n",
      "Epoch:581\t Cost:0.08911763067478373\n",
      "Epoch:582\t Cost:0.08593587998920349\n",
      "Epoch:583\t Cost:0.08901864246153501\n",
      "Epoch:584\t Cost:0.08525769999508918\n",
      "Epoch:585\t Cost:0.08761421890695022\n",
      "Epoch:586\t Cost:0.08463238175910737\n",
      "Epoch:587\t Cost:0.08310412364140121\n",
      "Epoch:588\t Cost:0.08566165151499482\n",
      "Epoch:589\t Cost:0.08882862432345526\n",
      "Epoch:590\t Cost:0.08786412585240755\n",
      "Epoch:591\t Cost:0.08692892920027283\n",
      "Epoch:592\t Cost:0.08633296262385494\n",
      "Epoch:593\t Cost:0.08641435961916184\n",
      "Epoch:594\t Cost:0.08418130448489174\n",
      "Epoch:595\t Cost:0.08566772729071027\n",
      "Epoch:596\t Cost:0.08699096416804328\n",
      "Epoch:597\t Cost:0.08326251732745468\n",
      "Epoch:598\t Cost:0.08506299325350378\n",
      "Epoch:599\t Cost:0.08606361150979869\n",
      "Epoch:600\t Cost:0.08543617081349847\n",
      "Epoch:601\t Cost:0.09085685062581605\n",
      "Epoch:602\t Cost:0.08582873026262532\n",
      "Epoch:603\t Cost:0.0842789690524562\n",
      "Epoch:604\t Cost:0.08298786074074029\n",
      "Epoch:605\t Cost:0.08654740355662636\n",
      "Epoch:606\t Cost:0.08771127404799975\n",
      "Epoch:607\t Cost:0.08605662962671981\n",
      "Epoch:608\t Cost:0.08403602184882678\n",
      "Epoch:609\t Cost:0.08256762935670772\n",
      "Epoch:610\t Cost:0.08557107824006124\n",
      "Epoch:611\t Cost:0.08861675995316651\n",
      "Epoch:612\t Cost:0.08488383114353808\n",
      "Epoch:613\t Cost:0.0834643452573242\n",
      "Epoch:614\t Cost:0.08378994241475995\n",
      "Epoch:615\t Cost:0.08764825631449344\n",
      "Epoch:616\t Cost:0.08296701013009304\n",
      "Epoch:617\t Cost:0.08633907995164902\n",
      "Epoch:618\t Cost:0.0869621450780456\n",
      "Epoch:619\t Cost:0.0861356969930724\n",
      "Epoch:620\t Cost:0.0832945311596907\n",
      "Epoch:621\t Cost:0.08322183144832852\n",
      "Epoch:622\t Cost:0.08555297262823658\n",
      "Epoch:623\t Cost:0.08546648120908722\n",
      "Epoch:624\t Cost:0.08349341914049789\n",
      "Epoch:625\t Cost:0.08831044605402931\n",
      "Epoch:626\t Cost:0.08340711875765054\n",
      "Epoch:627\t Cost:0.08307117922132266\n",
      "Epoch:628\t Cost:0.08317403963355939\n",
      "Epoch:629\t Cost:0.0842004909253578\n",
      "Epoch:630\t Cost:0.08633685582823782\n",
      "Epoch:631\t Cost:0.08455493466964345\n",
      "Epoch:632\t Cost:0.08478111409540497\n",
      "Epoch:633\t Cost:0.08509201548501119\n",
      "Epoch:634\t Cost:0.08413816851847716\n",
      "Epoch:635\t Cost:0.08139305177734478\n",
      "Epoch:636\t Cost:0.08383448781576683\n",
      "Epoch:637\t Cost:0.08466612324833489\n",
      "Epoch:638\t Cost:0.08330045741742481\n",
      "Epoch:639\t Cost:0.0867889363374283\n",
      "Epoch:640\t Cost:0.0818670176184063\n",
      "Epoch:641\t Cost:0.08142732222403672\n",
      "Epoch:642\t Cost:0.08289712374366677\n",
      "Epoch:643\t Cost:0.08506973339383726\n",
      "Epoch:644\t Cost:0.08498155450158472\n",
      "Epoch:645\t Cost:0.0833694321045807\n",
      "Epoch:646\t Cost:0.08374909356212629\n",
      "Epoch:647\t Cost:0.08377648118152611\n",
      "Epoch:648\t Cost:0.08458512346905622\n",
      "Epoch:649\t Cost:0.08465177254445518\n",
      "\n",
      "\n",
      "Training accuracy:0.9782000184059143\n",
      "Test accuracy:0.927299976348877\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# NORMALISE INPUTS\n",
    "x_test = mnist.test_images().reshape(10000,-1).T\n",
    "y_test = mnist.test_labels().reshape(-1,1).T\n",
    "y_test = one_hot_encode_labels(y_test,depth = 10)\n",
    "trained_parameters = model(X_train = x_train , Y_train = y_train, x_test=x_test, y_test=y_test, epochs = 650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
